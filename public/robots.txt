# robots.txt for akashbuilds.com  (2025-08-08)
# Purpose: Max discoverability in classic search + AI search, while opting out of training bots.

# --- Default policy ---
User-agent: *
Allow: /
Disallow: /admin/
Disallow: /private/
Disallow: /api/
Disallow: /cgi-bin/

Sitemap: https://akashbuilds.com/sitemap.xml

# --- Classic search engines (explicit allow) ---
User-agent: Googlebot
Allow: /
User-agent: Googlebot-Image
Allow: /
User-agent: Bingbot
Allow: /
User-agent: DuckDuckBot
Allow: /
User-agent: Applebot
Allow: /
# Brave says it may not always advertise a unique UA, but allow it when present:
User-agent: Bravebot
Allow: /

# --- AI search/retrieval crawlers (explicit allow) ---
# OpenAI web search indexer for ChatGPT results
User-agent: OAI-SearchBot
Allow: /
# Anthropic retrieval/search
User-agent: Claude-User
Allow: /
User-agent: Claude-SearchBot
Allow: /
# Perplexity search listing bot (not training)
User-agent: PerplexityBot
Allow: /
User-agent: Perplexity-User
Allow: /

# --- AI model training / broad corpus collectors (opt OUT by default) ---
# (Flip to Allow: / if you want to let training use your site.)
User-agent: GPTBot
Disallow: /
User-agent: ClaudeBot
Disallow: /
User-agent: Google-Extended
Disallow: /
User-agent: Applebot-Extended
Disallow: /
# Common Crawl often feeds training corpora:
User-agent: CCBot
Disallow: /

# --- Optional gentle rate limiting (only some bots respect this) ---
# Anthropic says they honor Crawl-delay; use if needed:
User-agent: ClaudeBot
Crawl-delay: 1
User-agent: Claude-SearchBot
Crawl-delay: 1
User-agent: Claude-User
Crawl-delay: 1
User-agent: *
Allow: /

# Sitemap
Sitemap: https://akashbuilds.com/sitemap.xml

# Crawl-delay
Crawl-delay: 1

# Disallow admin and private areas
Disallow: /admin/
Disallow: /private/
Disallow: /api/

# Allow important pages
Allow: /
Allow: /services/
Allow: /case-studies/
Allow: /about/
Allow: /contact/
Allow: /portfolio/ 